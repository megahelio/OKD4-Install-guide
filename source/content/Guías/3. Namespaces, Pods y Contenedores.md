
Uso ``oc get clusteroperatos`` se puede abreviar ``oc get co`` para saber si la *console* web está operativa (y el resto del cluster). La salida se parece a esto:
```
[root@odk4-services ~]# oc get co
NAME                                       VERSION             AVAILABLE   PROGRESSING   DEGRADED   SINCE   MESSAGE
authentication                             4.18.0-okd-scos.9   False       True          True       12h     OAuthServerRouteEndpointAccessibleControllerAvailable: failed to retrieve route from cache: route.route.openshift.io "oauth-openshift" not found...
baremetal                                  4.18.0-okd-scos.9   True        False         False      12h
cloud-controller-manager                   4.18.0-okd-scos.9   True        False         False      12h
cloud-credential                           4.18.0-okd-scos.9   True        False         False      13h
cluster-autoscaler                         4.18.0-okd-scos.9   True        False         False      12h
config-operator                            4.18.0-okd-scos.9   True        False         False      12h
console                                    4.18.0-okd-scos.9   False       False         True       12h     RouteHealthAvailable: console route is not admitted
control-plane-machine-set                  4.18.0-okd-scos.9   True        False         False      12h
csi-snapshot-controller                    4.18.0-okd-scos.9   True        False         False      12h
dns                                        4.18.0-okd-scos.9   True        False         False      12h
etcd                                       4.18.0-okd-scos.9   True        False         False      12h
image-registry                             4.18.0-okd-scos.9   True        False         False      12h
ingress                                    4.18.0-okd-scos.9   True        True          True       12h     The "default" ingress controller reports Degraded=True: DegradedConditions: One or more other status conditions indicate a degraded state: DeploymentReplicasAllAvailable=False (DeploymentReplicasNotAvailable: 0/1 of replicas are available), CanaryChecksSucceeding=Unknown (CanaryRouteNotAdmitted: Canary route is not admitted by the default ingress controller)
insights                                   4.18.0-okd-scos.9   True        False         False      12h
kube-apiserver                             4.18.0-okd-scos.9   True        False         False      12h
kube-controller-manager                    4.18.0-okd-scos.9   True        False         True       12h     GarbageCollectorDegraded: error fetching rules: Get "https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules": dial tcp 172.30.225.86:9091: connect: connection refused
kube-scheduler                             4.18.0-okd-scos.9   True        False         False      12h
kube-storage-version-migrator              4.18.0-okd-scos.9   True        False         False      12h
machine-api                                4.18.0-okd-scos.9   True        False         False      12h
machine-approver                           4.18.0-okd-scos.9   True        False         False      12h
machine-config                                                 True        True          True       12h     Unable to apply 4.18.0-okd-scos.9: error during waitForDaemonsetRollout: [context deadline exceeded, daemonset machine-config-daemon is not ready. status: (desired: 2, updated: 2, ready: 0, unavailable: 2)]
marketplace                                4.18.0-okd-scos.9   True        False         False      12h
monitoring                                                     False       True          True       12h     UpdatingPrometheusOperator: reconciling Prometheus Operator Admission Webhook Deployment failed: updating Deployment object failed: waiting for DeploymentRollout of openshift-monitoring/prometheus-operator-admission-webhook: context deadline exceeded: got 1 unavailable replicas
network                                    4.18.0-okd-scos.9   True        False         False      12h
node-tuning                                4.18.0-okd-scos.9   True        False         False      10m
olm                                        4.18.0-okd-scos.9   True        False         False      12h
openshift-apiserver                        4.18.0-okd-scos.9   True        False         False      12h
openshift-controller-manager               4.18.0-okd-scos.9   True        False         False      12h
openshift-samples                          4.18.0-okd-scos.9   True        False         False      12h
operator-lifecycle-manager                 4.18.0-okd-scos.9   True        False         False      12h
operator-lifecycle-manager-catalog         4.18.0-okd-scos.9   True        False         False      12h
operator-lifecycle-manager-packageserver   4.18.0-okd-scos.9   True        False         False      12h
service-ca                                 4.18.0-okd-scos.9   True        False         False      12h
storage                                    4.18.0-okd-scos.9   True        False         False      12h
```

La salida ideal sería todo el Available a True y el resto a false. Para explorar qué esta pasando en cada *namespace* podemos usar:

```
oc get pods -n <NAMESPACE>
```

> [!NOTE]
> -  El comando ``oc get namespaces`` devuelve una lista de todos los namespaces que se usan. 
> -  Si miramos la salida de ``oc get co`` el namespace del clusteroperator se suele corresponder con **openshift-{name del co}**.

En la salida de ``oc get pods -n openshift-kube-controller-manager`` vemos que hay un pod con errores.

```
NAME                                                         READY   STATUS      RESTARTS   AGE
installer-3-okd4-control-plane-1.lab.okd.local               0/1     Completed   0          13h
installer-4-okd4-control-plane-1.lab.okd.local               0/1     Completed   0          13h
installer-5-okd4-control-plane-1.lab.okd.local               0/1     Error       0          13h
installer-6-okd4-control-plane-1.lab.okd.local               0/1     Completed   0          12h
kube-controller-manager-okd4-control-plane-1.lab.okd.local   4/4     Running     0          12h
```

Para ver los logs del pod que falla: 

```
oc logs -n openshift-kube-controller-manager installer-5-okd4-control-plane-1.lab.okd.local
```

> [!NOTE] 
> - **NAMESPACE**: ``openshift-kube-controller-manager``
> - **POD**:  ``installer-5-okd4-control-plane-1.lab.okd.local``

En mi proceso de instalación me encontré con los siguientes errores:

[[Si el openshift-ingress router-default se pone tonto]]
[[Si el oauth-openshift  openshift-authentication se pone tonto]]


|                      Previo                       | Siguiente  |
| :-----------------------------------------------: | :--------: |
| [[2. Instalación VMware imitando baremetal OKD4]] | [[4. Nfs]] |
